{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homomorphic Encryption in CNN Based Intrusion Detection System for Internet of Vehicles \n",
    "This is the code for the paper entitled \"**Homomorphic Encryption in CNN Based Intrusion Detection System for Internet of Vehicles**\"\n",
    "Authors: Ronilson Candido do Nascimento (ronilson.nascimento@laccan.ufal.br), Douglas Moura (douglas.moura@dcc.ufmg.br) and André Aquino (alla@laccan.ufal.br)  \n",
    "Organization: Instituto de Computação - Universidade Federal de Alagoas (UFAL); Departamento de Ciência da Computação Universidade Federal de Minas Gerais (UFMG)\n",
    "\n",
    "**Notebook 2: CNN Model Development**  \n",
    "Aims:  \n",
    "&nbsp; 1): Generate training and test images  \n",
    "&nbsp; 2): Construct CNN models (a CNN model by own, Xception, VGG16, VGG19, Resnet, Inception, InceptionResnet)  \n",
    "&nbsp; 3): Tune the hyperparameters of CNN models (hyperparameter optimization)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense,Flatten,GlobalAveragePooling2D,Input,Conv2D,MaxPooling2D,Dropout\n",
    "from keras.models import Model,load_model,Sequential\n",
    "from keras.applications.xception import  Xception\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import  ResNet50\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import keras.callbacks as kcallbacks\n",
    "import keras\n",
    "from keras.preprocessing.image import load_img,img_to_array\n",
    "import math\n",
    "import random\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Training and Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23383 images belonging to 5 classes.\n",
      "Found 5845 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "#generate training and test images\n",
    "TARGET_SIZE=(224,224)\n",
    "INPUT_SIZE=(224,224,3)\n",
    "BATCHSIZE=128\t#could try 128 or 32\n",
    "\n",
    "#Normalization\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './train_224/',\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCHSIZE,\n",
    "        class_mode='categorical')\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        './test_224/',\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCHSIZE,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the image plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot the figures\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # acc\n",
    "            plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "            # loss\n",
    "            plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history_this= LossHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct CNN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Model 1: a CNN model by own (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_by_own(input_shape, num_class, epochs, savepath='./model_own.keras'):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), input_shape=input_shape, padding='same', activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(num_class, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(monitor='val_accuracy', patience=2, verbose=1, mode='auto')\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(filepath=savepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "    hist = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        callbacks=[earlyStopping, saveBestModel, history_this],\n",
    "    )\n",
    "\n",
    "    return hist  # Certifique-se de retornar o histórico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_by_own(input_shape=INPUT_SIZE,num_class=5,epochs=20)\n",
    "history_this.loss_plot('epoch')\n",
    "history_this.loss_plot('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Validation accuracy of a CNN by own: 99.884%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xception(num_class, epochs, savepath='./xception.keras', history=history_this, input_shape=INPUT_SIZE):\n",
    "    model_fine_tune = Xception(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    \n",
    "    # Congelar camadas\n",
    "    for layer in model_fine_tune.layers[:131]:\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[131:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Adicionar camadas\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model = Dense(units=256, activation='relu')(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='xception')\n",
    "    \n",
    "    # Definir o otimizador com o parâmetro correto\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    # Treinar o modelo\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=3, verbose=1, mode='auto')\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "    \n",
    "    hist = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        callbacks=[earlyStopping, saveBestModel, history]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\migue\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59s/step - accuracy: 0.9549 - loss: 0.1224 "
     ]
    }
   ],
   "source": [
    "# Chamar a função xception\n",
    "xception(num_class=5, epochs=20)\n",
    "\n",
    "# Plotar os gráficos de perda\n",
    "history_this.loss_plot('epoch')\n",
    "history_this.loss_plot('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy of Xception: 100.0%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg16( num_class, epochs,savepath='./VGG16.h5',history=history_this,input_shape=INPUT_SIZE):\n",
    "    model_fine_tune = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:15]:\t#the number of frozen layers for transfer learning, have tuned from 5-18\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[15:]:\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output) #GlobalAveragePooling2D layer to convert the features to a single 1280-element vector per image\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(0.5)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='vgg')\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\t#set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=2, verbose=1, mode='auto')\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        #workers=2,\n",
    "        callbacks=[earlyStopping, saveBestModel, history],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg16(num_class=5,epochs=20)\t#tf36cnn\n",
    "history_this.loss_plot('epoch')\n",
    "history_this.loss_plot('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Validation accuracy of VGG16: 100.0%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg19( num_class, epochs,savepath='./VGG19.h5',history=history_this,input_shape=INPUT_SIZE):\n",
    "    model_fine_tune = VGG19(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:19]:\t#the number of frozen layers for transfer learning, have tuned from 5-18\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[19:]:\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(0.5)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='vgg')\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\t#set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=2, verbose=1, mode='auto')\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        #workers=2,\n",
    "        callbacks=[earlyStopping, saveBestModel, history],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg19(num_class=5,epochs=20)\t#binary classificaiton\n",
    "history_this.loss_plot('epoch')\n",
    "history_this.loss_plot('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Validation accuracy of VGG19: 100.0%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnet( num_class, epochs,savepath='./resnet.h5',history=history_this,input_shape=INPUT_SIZE):\n",
    "    model_fine_tune = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:120]:\t#the number of frozen layers for transfer learning, have tuned from 50-150\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[120:]:\t#the number of trainable layers for transfer learning\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(0.5)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='resnet')\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) #set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=2, verbose=1, mode='auto')\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        callbacks=[earlyStopping, saveBestModel, history],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resnet(num_class=5,epochs=20)\t#binary classificaiton\n",
    "history_this.loss_plot('epoch')\n",
    "history_this.loss_plot('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy of Resnet: 98.652%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inception( num_class, epochs,savepath='./inception.h5',history=history_this,input_shape=INPUT_SIZE):\n",
    "    model_fine_tune = InceptionV3(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:35]:\t#the number of frozen layers for transfer learning, have tuned from 50-150\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[35:]:\t#the number of trainable layers for transfer learning\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(0.5)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='resnet')\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) #set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=2, verbose=1, mode='auto')\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        callbacks=[earlyStopping, saveBestModel, history],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inception(num_class=5,epochs=20)\t#binary classificaiton\n",
    "history_this.loss_plot('epoch')\n",
    "history_this.loss_plot('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy of Inception: 100.0%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7: InceptionResnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inceptionresnet( num_class, epochs,savepath='./inceptionresnet.h5',history=history_this,input_shape=INPUT_SIZE):\n",
    "    model_fine_tune = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:500]:\t#the number of frozen layers for transfer learning, have tuned from 400-550\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[500:]:\t#the number of trainable layers for transfer learning\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(0.5)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='resnet')\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) #set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=2, verbose=1, mode='auto')\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        callbacks=[earlyStopping, saveBestModel, history],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inceptionresnet(num_class=5,epochs=20)\t# 5-class classificaiton\n",
    "history_this.loss_plot('epoch')\n",
    "history_this.loss_plot('batch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Validation accuracy of InceptionResnet: 99.993%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization \n",
    "Use VGG16 as an example.  \n",
    "\n",
    "Tuned hyperparameters of CNN: \n",
    "1. The number of frozen layers\n",
    "2. The number of epochs\n",
    "3. Early stop patience\n",
    "4. Learning rate\n",
    "5. Dropout rate\n",
    "\n",
    "Hyperparameter optimization methods:\n",
    "1. Random search\n",
    "2. Bayesian optimization - Tree Parzen Estimator(BO-TPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vgg16( num_class,epochs=20,frozen=15,lr=0.001,patience=2, dropout_rate=0.5,verbose=0, savepath='./VGG16.h5',history=history_this,input_shape=INPUT_SIZE):\n",
    "    model_fine_tune = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    for layer in model_fine_tune.layers[:frozen]:\t#the number of frozen layers for transfer learning, have tuned from 5-18\n",
    "        layer.trainable = False\n",
    "    for layer in model_fine_tune.layers[frozen:]:\n",
    "        layer.trainable = True\n",
    "    model = GlobalAveragePooling2D()(model_fine_tune.output)\n",
    "    model=Dense(units=256,activation='relu')(model)\n",
    "    model=Dropout(dropout_rate)(model)\n",
    "    model = Dense(num_class, activation='softmax')(model)\n",
    "    model = Model(model_fine_tune.input, model, name='vgg')\n",
    "    opt = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\t#tuned learning rate to be 0.001\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\t#set the loss function to be binary crossentropy\n",
    "    #train model\n",
    "    earlyStopping = kcallbacks.EarlyStopping(\n",
    "        monitor='val_acc', patience=patience, verbose=verbose, mode='auto')\t#set early stop patience to save training time\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(\n",
    "        filepath=savepath,\n",
    "        monitor='val_acc',\n",
    "        verbose=verbose,\n",
    "        save_best_only=True,\n",
    "        mode='auto')\n",
    "    hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        #use_multiprocessing=True, \n",
    "        #workers=2,\n",
    "        callbacks=[earlyStopping, saveBestModel, history],\n",
    "        verbose = verbose\n",
    "    )\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction(vgg_model):\n",
    "#read images from validation folder\n",
    "    rootdir = './test_224/'\n",
    "    test_laels = []\n",
    "    test_images=[]\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            if not (file.endswith(\".jpeg\"))|(file.endswith(\".jpg\"))|(file.endswith(\".png\")):\n",
    "                continue\n",
    "            test_laels.append(subdir.split('/')[-1])\n",
    "            test_images.append(os.path.join(subdir, file))\n",
    "\n",
    "    predict=[]\n",
    "    length=len(test_images)\n",
    "    label=validation_generator.class_indices\n",
    "    label={v: k for k, v in label.items()}\n",
    "    for i in range(length):\n",
    "        inputimg=test_images[i]\n",
    "        test_batch=[]\n",
    "        thisimg=np.array(Image.open(inputimg))/255 #read all the images in validation set\n",
    "        #print(thisimg)\n",
    "        test_shape=(1,)+thisimg.shape\n",
    "        thisimg=thisimg.reshape(test_shape)\n",
    "        vgg_model_batch=vgg_model.predict(thisimg) #use master model to process the input image\n",
    "        #generate result by model 1\n",
    "        prob=vgg_model_batch[0,np.argmax(vgg_model_batch,axis=1)[0]]\n",
    "        res=label[np.argmax(vgg_model_batch,axis=1)[0]]\n",
    "        predict.append(res)\n",
    "    acc=accuracy_score(test_laels,predict)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define the objective function to be optimized\n",
    "import time\n",
    "from hyperopt import hp, fmin, tpe, rand, STATUS_OK, Trials\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics \n",
    "\n",
    "def objective(params):\n",
    "    \n",
    "    params = {\n",
    "        'frozen': int(params['frozen']),\n",
    "        'epochs': int(params['epochs']),\n",
    "        'patience': int(params['patience']),\n",
    "        'lr': abs(float(params['lr'])),\n",
    "        'dropout_rate': abs(float(params['dropout_rate'])),\n",
    "    }\n",
    "    frozen=params['frozen']\n",
    "    epochs=params['epochs']\n",
    "    patience=params['patience']\n",
    "    lr=params['lr']\n",
    "    dropout_rate=params['dropout_rate']\n",
    "\n",
    "    vgg16(num_class=5, frozen=frozen,epochs=epochs,patience=patience, lr=lr, dropout_rate=dropout_rate)\n",
    "\n",
    "    acc=prediction(vgg_model=load_model('./VGG16.h5'))\n",
    "\n",
    "    print('accuracy:%s'%acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Bayesian optimization - Tree Parzen Estimator\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 15, 18, 1),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Hyperparameter optimization by Random search\n",
    "space = {\n",
    "    'frozen': hp.quniform('frozen', 15, 18, 1),\n",
    "    'epochs': hp.quniform('epochs', 5, 21, 5),\n",
    "    'patience': hp.quniform('patience', 2, 4, 1),\n",
    "    'lr': hp.quniform('lr', 0.001, 0.006, 0.001),\n",
    "    'dropout_rate': hp.quniform('dropout_rate', 0.3, 0.6, 0.1),\n",
    "}\n",
    "\n",
    "t1=time.time()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=rand.suggest,\n",
    "            max_evals=10)\n",
    "print(\"Hyperopt estimated optimum {}\".format(best))\n",
    "t2=time.time()\n",
    "print(\"Time: \"+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrain the model by using the best hyperparameter values to obtain the best model\n",
    "vgg16(num_class=5, frozen=16,epochs=15,patience=3, lr=0.002, dropout_rate=0.6,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
