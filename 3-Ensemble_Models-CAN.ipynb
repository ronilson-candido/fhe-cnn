{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Transfer Learning and Optimized CNN Based Intrusion Detection System for Internet of Vehicles \n",
    "This is the code for the paper entitled \"**A Transfer Learning and Optimized CNN Based Intrusion Detection System for Internet of Vehicles**\" published in **IEEE International Conference on Communications (IEEE ICC)**, doi=[10.1109/ICC45855.2022.9838780](https://ieeexplore.ieee.org/document/9838780).    \n",
    "Authors: Li Yang (lyang339@uwo.ca) and Abdallah Shami (Abdallah.Shami@uwo.ca)  \n",
    "Organization: The Optimized Computing and Communications (OC2) Lab, ECE Department, Western University\n",
    "\n",
    "**Notebook 3: Ensemble Models**  \n",
    "Aims:  construct three ensemble techniques: Bagging, Probability averaging, and Concatenation, to further improve prediction accuracy  \n",
    "* Bagging: use majority voting of top single models  \n",
    "* Probability averaging: calculate the average probability of the single model prediction results (the last layer of CNN models), and select the largest probability class to be the final class  \n",
    "* Concatenation: extract the features in the last several layers of single models, and concatenate together to generate the new layers, and add a dense layer to do prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import keras\n",
    "from keras.models import Model,load_model\n",
    "from keras import Input\n",
    "from keras.layers import concatenate,Dense,Flatten,Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import keras.callbacks as kcallbacks\n",
    "import os\n",
    "import math\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.optimizers import SGD\n",
    "import operator\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5845 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "#generate images from train set and validation set\n",
    "TARGET_SIZE=(224,224)\n",
    "INPUT_SIZE=(224,224,3)\n",
    "BATCHSIZE=128\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        './test_224/',\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCHSIZE,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate labels indicating disease (1) or normal (0)\n",
    "label=validation_generator.class_indices\n",
    "label={v: k for k, v in label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '0', 1: '1', 2: '2', 3: '3', 4: '4'}\n"
     ]
    }
   ],
   "source": [
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ./test_224/0\\100183.png\n"
     ]
    }
   ],
   "source": [
    "#read images from validation folder\n",
    "rootdir = './test_224/'\n",
    "test_laels = []\n",
    "test_images=[]\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        if not (file.endswith(\".jpeg\"))|(file.endswith(\".jpg\"))|(file.endswith(\".png\")):\n",
    "            continue\n",
    "        test_laels.append(subdir.split('/')[-1])\n",
    "        test_images.append(os.path.join(subdir, file))\n",
    "        \n",
    "print(test_laels[0],test_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 5 trained CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " #load model 1: xception\n",
    "xception_model=load_model('./xception.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #load model 2: VGG16\n",
    "vgg_model=load_model('./VGG16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #load model 3: VGG19\n",
    "vgg19_model=load_model('./VGG19.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #load model 4: inception\n",
    "incep_model=load_model('./inception.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #load model 5: inceptionresnet\n",
    "inres_model=load_model('./inceptionresnet.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the original CNN base models to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Single image prediction\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "test=cv2.imread(test_images[0])\n",
    "\n",
    "img_show=test[:,:,[2,1,0]]\n",
    "test=test/255.\n",
    "test_shape=(1,)+test.shape\n",
    "test=test.reshape(test_shape)\n",
    "\n",
    "res=xception_model.predict(test)\n",
    "\n",
    "prob=res[0,np.argmax(res,axis=1)[0]]\n",
    "res=label[np.argmax(res,axis=1)[0]]\n",
    "print('Predicted result for the first image: %s'%res)\n",
    "print('Confidence level: %s'%prob)\n",
    "plt.imshow(img_show)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "predict=[]\n",
    "length=len(test_images)\n",
    "t1 = time.time()\n",
    "for i in range(length):\n",
    "    inputimg=test_images[i]\n",
    "    test_batch=[]\n",
    "    thisimg=np.array(Image.open(inputimg))/255 #read all the images in validation set\n",
    "    #print(thisimg)\n",
    "    test_shape=(1,)+thisimg.shape\n",
    "    thisimg=thisimg.reshape(test_shape)\n",
    "    xception_model_batch=xception_model.predict(thisimg) #use master model to process the input image\n",
    "    #generate result by model 1\n",
    "    prob=xception_model_batch[0,np.argmax(xception_model_batch,axis=1)[0]]\n",
    "    res=label[np.argmax(xception_model_batch,axis=1)[0]]\n",
    "    predict.append(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xception accuracy: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "f1: 1.0\n",
      "[[5052    0    0    0    0]\n",
      " [   0  225    0    0    0]\n",
      " [   0    0  200    0    0]\n",
      " [   0    0    0  197    0]\n",
      " [   0    0    0    0  171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5052\n",
      "           1       1.00      1.00      1.00       225\n",
      "           2       1.00      1.00      1.00       200\n",
      "           3       1.00      1.00      1.00       197\n",
      "           4       1.00      1.00      1.00       171\n",
      "\n",
      "    accuracy                           1.00      5845\n",
      "   macro avg       1.00      1.00      1.00      5845\n",
      "weighted avg       1.00      1.00      1.00      5845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "acc=accuracy_score(test_laels,predict)\n",
    "pre=precision_score(test_laels,predict,average='weighted')\n",
    "re=recall_score(test_laels,predict,average='weighted')\n",
    "f1=f1_score(test_laels,predict,average='weighted')\n",
    "print('Xception accuracy: %s'%acc)\n",
    "print('precision: %s'%pre)\n",
    "print('recall: %s'%re)\n",
    "print('f1: %s'%f1)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_laels, predict))\n",
    "target_names = ['0', '1','2','3','4']\n",
    "print(classification_report(test_laels, predict, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 50 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predict=[]\n",
    "length=len(test_images)\n",
    "t1 = time.time()\n",
    "for i in range(length):\n",
    "    inputimg=test_images[i]\n",
    "    test_batch=[]\n",
    "    thisimg=np.array(Image.open(inputimg))/255 #read all the images in validation set\n",
    "    #print(thisimg)\n",
    "    test_shape=(1,)+thisimg.shape\n",
    "    thisimg=thisimg.reshape(test_shape)\n",
    "    vgg_model_batch=vgg_model.predict(thisimg) #use master model to process the input image\n",
    "    #generate result by model 1\n",
    "    prob=vgg_model_batch[0,np.argmax(vgg_model_batch,axis=1)[0]]\n",
    "    res=label[np.argmax(vgg_model_batch,axis=1)[0]]\n",
    "    predict.append(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16 accuracy: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "f1: 1.0\n",
      "[[5052    0    0    0    0]\n",
      " [   0  225    0    0    0]\n",
      " [   0    0  200    0    0]\n",
      " [   0    0    0  197    0]\n",
      " [   0    0    0    0  171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5052\n",
      "           1       1.00      1.00      1.00       225\n",
      "           2       1.00      1.00      1.00       200\n",
      "           3       1.00      1.00      1.00       197\n",
      "           4       1.00      1.00      1.00       171\n",
      "\n",
      "    accuracy                           1.00      5845\n",
      "   macro avg       1.00      1.00      1.00      5845\n",
      "weighted avg       1.00      1.00      1.00      5845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "acc=accuracy_score(test_laels,predict)\n",
    "pre=precision_score(test_laels,predict,average='weighted')\n",
    "re=recall_score(test_laels,predict,average='weighted')\n",
    "f1=f1_score(test_laels,predict,average='weighted')\n",
    "print('VGG16 accuracy: %s'%acc)\n",
    "print('precision: %s'%pre)\n",
    "print('recall: %s'%re)\n",
    "print('f1: %s'%f1)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_laels, predict))\n",
    "target_names = ['0', '1','2','3','4']\n",
    "print(classification_report(test_laels, predict, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 56.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predict=[]\n",
    "length=len(test_images)\n",
    "t1 = time.time()\n",
    "for i in range(length):\n",
    "    inputimg=test_images[i]\n",
    "    test_batch=[]\n",
    "    thisimg=np.array(Image.open(inputimg))/255 #read all the images in validation set\n",
    "    #print(thisimg)\n",
    "    test_shape=(1,)+thisimg.shape\n",
    "    thisimg=thisimg.reshape(test_shape)\n",
    "    vgg19_model_batch=vgg19_model.predict(thisimg) #use master model to process the input image\n",
    "    #generate result by model 1\n",
    "    prob=vgg19_model_batch[0,np.argmax(vgg19_model_batch,axis=1)[0]]\n",
    "    res=label[np.argmax(vgg19_model_batch,axis=1)[0]]\n",
    "    predict.append(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG19 accuracy: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "f1: 1.0\n",
      "[[5052    0    0    0    0]\n",
      " [   0  225    0    0    0]\n",
      " [   0    0  200    0    0]\n",
      " [   0    0    0  197    0]\n",
      " [   0    0    0    0  171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5052\n",
      "           1       1.00      1.00      1.00       225\n",
      "           2       1.00      1.00      1.00       200\n",
      "           3       1.00      1.00      1.00       197\n",
      "           4       1.00      1.00      1.00       171\n",
      "\n",
      "    accuracy                           1.00      5845\n",
      "   macro avg       1.00      1.00      1.00      5845\n",
      "weighted avg       1.00      1.00      1.00      5845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "acc=accuracy_score(test_laels,predict)\n",
    "pre=precision_score(test_laels,predict,average='weighted')\n",
    "re=recall_score(test_laels,predict,average='weighted')\n",
    "f1=f1_score(test_laels,predict,average='weighted')\n",
    "print('VGG19 accuracy: %s'%acc)\n",
    "print('precision: %s'%pre)\n",
    "print('recall: %s'%re)\n",
    "print('f1: %s'%f1)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_laels, predict))\n",
    "target_names = ['0', '1','2','3','4']\n",
    "print(classification_report(test_laels, predict, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predict=[]\n",
    "length=len(test_images)\n",
    "t1 = time.time()\n",
    "for i in range(length):\n",
    "    inputimg=test_images[i]\n",
    "    test_batch=[]\n",
    "    thisimg=np.array(Image.open(inputimg))/255 #read all the images in validation set\n",
    "    #print(thisimg)\n",
    "    test_shape=(1,)+thisimg.shape\n",
    "    thisimg=thisimg.reshape(test_shape)\n",
    "    incep_model_batch=incep_model.predict(thisimg) #use master model to process the input image\n",
    "    #generate result by model 1\n",
    "    prob=incep_model_batch[0,np.argmax(incep_model_batch,axis=1)[0]]\n",
    "    res=label[np.argmax(incep_model_batch,axis=1)[0]]\n",
    "    predict.append(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inception accuracy: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "f1: 1.0\n",
      "[[5052    0    0    0    0]\n",
      " [   0  225    0    0    0]\n",
      " [   0    0  200    0    0]\n",
      " [   0    0    0  197    0]\n",
      " [   0    0    0    0  171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5052\n",
      "           1       1.00      1.00      1.00       225\n",
      "           2       1.00      1.00      1.00       200\n",
      "           3       1.00      1.00      1.00       197\n",
      "           4       1.00      1.00      1.00       171\n",
      "\n",
      "    accuracy                           1.00      5845\n",
      "   macro avg       1.00      1.00      1.00      5845\n",
      "weighted avg       1.00      1.00      1.00      5845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "acc=accuracy_score(test_laels,predict)\n",
    "pre=precision_score(test_laels,predict,average='weighted')\n",
    "re=recall_score(test_laels,predict,average='weighted')\n",
    "f1=f1_score(test_laels,predict,average='weighted')\n",
    "print('inception accuracy: %s'%acc)\n",
    "print('precision: %s'%pre)\n",
    "print('recall: %s'%re)\n",
    "print('f1: %s'%f1)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_laels, predict))\n",
    "target_names = ['0', '1','2','3','4']\n",
    "print(classification_report(test_laels, predict, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. InceptionResnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predict=[]\n",
    "length=len(test_images)\n",
    "t1 = time.time()\n",
    "for i in range(length):\n",
    "    inputimg=test_images[i]\n",
    "    test_batch=[]\n",
    "    thisimg=np.array(Image.open(inputimg))/255 #read all the images in validation set\n",
    "    #print(thisimg)\n",
    "    test_shape=(1,)+thisimg.shape\n",
    "    thisimg=thisimg.reshape(test_shape)\n",
    "    inres_model_batch=inres_model.predict(thisimg) #use master model to process the input image\n",
    "    #generate result by model 1\n",
    "    prob=inres_model_batch[0,np.argmax(inres_model_batch,axis=1)[0]]\n",
    "    res=label[np.argmax(inres_model_batch,axis=1)[0]]\n",
    "    predict.append(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inceptionresnet accuracy: 0.9998289136013687\n",
      "precision: 0.999829777674089\n",
      "recall: 0.9998289136013687\n",
      "f1: 0.9998288793066084\n",
      "[[5052    0    0    0    0]\n",
      " [   0  225    0    0    0]\n",
      " [   0    0  200    0    0]\n",
      " [   0    0    0  197    0]\n",
      " [   0    0    0    1  170]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5052\n",
      "           1       1.00      1.00      1.00       225\n",
      "           2       1.00      1.00      1.00       200\n",
      "           3       0.99      1.00      1.00       197\n",
      "           4       1.00      0.99      1.00       171\n",
      "\n",
      "    accuracy                           1.00      5845\n",
      "   macro avg       1.00      1.00      1.00      5845\n",
      "weighted avg       1.00      1.00      1.00      5845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "acc=accuracy_score(test_laels,predict)\n",
    "pre=precision_score(test_laels,predict,average='weighted')\n",
    "re=recall_score(test_laels,predict,average='weighted')\n",
    "f1=f1_score(test_laels,predict,average='weighted')\n",
    "print('inceptionresnet accuracy: %s'%acc)\n",
    "print('precision: %s'%pre)\n",
    "print('recall: %s'%re)\n",
    "print('f1: %s'%f1)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_laels, predict))\n",
    "target_names = ['0', '1','2','3','4']\n",
    "print(classification_report(test_laels, predict, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 6. Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #load model 6: resnet\n",
    "res_model=load_model('./resnet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predict=[]\n",
    "length=len(test_images)\n",
    "t1 = time.time()\n",
    "for i in range(length):\n",
    "    inputimg=test_images[i]\n",
    "    test_batch=[]\n",
    "    thisimg=np.array(Image.open(inputimg))/255 #read all the images in validation set\n",
    "    #print(thisimg)\n",
    "    test_shape=(1,)+thisimg.shape\n",
    "    thisimg=thisimg.reshape(test_shape)\n",
    "    res_model_batch=res_model.predict(thisimg) #use master model to process the input image\n",
    "    #generate result by model 1\n",
    "    prob=res_model_batch[0,np.argmax(res_model_batch,axis=1)[0]]\n",
    "    res=label[np.argmax(res_model_batch,axis=1)[0]]\n",
    "    predict.append(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet accuracy: 0.9662959794696322\n",
      "precision: 0.9338569031275825\n",
      "recall: 0.9662959794696322\n",
      "f1: 0.9497662530625438\n",
      "[[5052    0    0    0    0]\n",
      " [   0  225    0    0    0]\n",
      " [   0    0  200    0    0]\n",
      " [ 197    0    0    0    0]\n",
      " [   0    0    0    0  171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      5052\n",
      "           1       1.00      1.00      1.00       225\n",
      "           2       1.00      1.00      1.00       200\n",
      "           3       0.00      0.00      0.00       197\n",
      "           4       1.00      1.00      1.00       171\n",
      "\n",
      "    accuracy                           0.97      5845\n",
      "   macro avg       0.79      0.80      0.80      5845\n",
      "weighted avg       0.93      0.97      0.95      5845\n",
      "\n",
      "Wall time: 35.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "acc=accuracy_score(test_laels,predict)\n",
    "pre=precision_score(test_laels,predict,average='weighted')\n",
    "re=recall_score(test_laels,predict,average='weighted')\n",
    "f1=f1_score(test_laels,predict,average='weighted')\n",
    "print('resnet accuracy: %s'%acc)\n",
    "print('precision: %s'%pre)\n",
    "print('recall: %s'%re)\n",
    "print('f1: %s'%f1)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_laels, predict))\n",
    "target_names = ['0', '1','2','3','4']\n",
    "print(classification_report(test_laels, predict, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best performing single model (vgg):  \n",
    "Accuracy: 99.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing time is :99.662228 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "predict=[]\n",
    "length=len(test_images)\n",
    "t1 = time.time()\n",
    "for i in range((length//127)+1):\n",
    "    inputimg=test_images[127*i:127*(i+1)]\n",
    "    test_batch=[]\n",
    "    for path in inputimg:\n",
    "        thisimg=np.array(Image.open(path))/255\n",
    "        test_batch.append(thisimg)\n",
    "    #generate result by model 1\n",
    "    xception_model_batch=xception_model.predict(np.array(test_batch))\n",
    "    xception_model_batch=list(np.argmax(xception_model_batch,axis=1))\n",
    "    xception_model_batch=[label[con] for con in xception_model_batch]\n",
    "#     print(xception_model_batch)\n",
    "    #generate result by model 2\n",
    "    vgg_model_batch=vgg_model.predict(np.array(test_batch))\n",
    "    vgg_model_batch=list(np.argmax(vgg_model_batch,axis=1))\n",
    "    vgg_model_batch=[label[con] for con in vgg_model_batch]\n",
    "#     print(vgg_model_batch)\n",
    "    #generate result by model 3\n",
    "    vgg19_model_batch=vgg19_model.predict(np.array(test_batch))\n",
    "    vgg19_model_batch=list(np.argmax(vgg19_model_batch,axis=1))\n",
    "    vgg19_model_batch=[label[con] for con in vgg19_model_batch]\n",
    "#     print(vgg19_model_batch)\n",
    "    #generate result by model 4\n",
    "    incep_model_batch=incep_model.predict(np.array(test_batch))\n",
    "    incep_model_batch=list(np.argmax(incep_model_batch,axis=1))\n",
    "    incep_model_batch=[label[con] for con in incep_model_batch]\n",
    "#     print(incep_model_batch)\n",
    "    #generate result by model 5\n",
    "    inres_model_batch=inres_model.predict(np.array(test_batch))\n",
    "    inres_model_batch=list(np.argmax(inres_model_batch,axis=1))\n",
    "    inres_model_batch=[label[con] for con in inres_model_batch]\n",
    "#     print(inres_model_batch)\n",
    "    #bagging the three results generated by 3 singular models\n",
    "    predict_batch=[]\n",
    "    for i,j,k,p,q in zip(xception_model_batch,vgg_model_batch,vgg19_model_batch,incep_model_batch,inres_model_batch):\n",
    "        count=defaultdict(int)\n",
    "        count[i]+=1\n",
    "        count[j]+=1\n",
    "        count[k]+=1\n",
    "        count[p]+=1\n",
    "        count[q]+=1\n",
    "        #rank the predicted results in descending order\n",
    "        predict_one=sorted(count.items(), key=operator.itemgetter(1),reverse=True)[0][0]\n",
    "        predict_batch.append(predict_one)\n",
    "#     print('predict:',predict_batch)\n",
    "    predict.append(predict_batch)\n",
    "t2 = time.time()\n",
    "print('The testing time is :%f seconds' % (t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict=sum(predict,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bagging accuracy:1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "acc=accuracy_score(test_laels,predict)\n",
    "print('bagging accuracy:%s'%acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5052    0    0    0    0]\n",
      " [   0  225    0    0    0]\n",
      " [   0    0  200    0    0]\n",
      " [   0    0    0  197    0]\n",
      " [   0    0    0    0  171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5052\n",
      "           1       1.00      1.00      1.00       225\n",
      "           2       1.00      1.00      1.00       200\n",
      "           3       1.00      1.00      1.00       197\n",
      "           4       1.00      1.00      1.00       171\n",
      "\n",
      "    accuracy                           1.00      5845\n",
      "   macro avg       1.00      1.00      1.00      5845\n",
      "weighted avg       1.00      1.00      1.00      5845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_laels, predict))\n",
    "target_names = ['0', '1','2','3','4']\n",
    "print(classification_report(test_laels, predict, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After bagging ensemble, the accuracy improved to 0.990"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Probability Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model,load_model\n",
    "from keras import Input\n",
    "from keras.layers import concatenate,Dense,Flatten,Dropout,Average\n",
    "from keras.preprocessing.image import  ImageDataGenerator\n",
    "import keras.callbacks as kcallbacks\n",
    "import os\n",
    "import math\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.optimizers import SGD\n",
    "import operator\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing time is :2.661651 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "img=Input(shape=(224,224,3),name='img')\n",
    "feature1=xception_model(img)\n",
    "feature2=vgg_model(img)\n",
    "feature3=incep_model(img)\n",
    "for layer in xception_model.layers:  \n",
    "    layer.trainable = False \n",
    "for layer in vgg_model.layers:  \n",
    "    layer.trainable = False  \n",
    "for layer in incep_model.layers:  \n",
    "    layer.trainable = False  \n",
    "output=Average()([feature1,feature2,feature3]) #add the confidence lists generated by 3 models\n",
    "model=Model(inputs=img,outputs=output)\n",
    "\n",
    "#the optimization function\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "t2 = time.time()\n",
    "print('The testing time is :%f seconds' % (t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ./test_224/0\\100015.png\n"
     ]
    }
   ],
   "source": [
    "#read images from validation folder\n",
    "rootdir = './test_224/'\n",
    "test_laels = []\n",
    "test_images=[]\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        if not (file.endswith(\".jpeg\"))|(file.endswith(\".jpg\"))|(file.endswith(\".png\")):\n",
    "            continue\n",
    "        test_laels.append(subdir.split('/')[-1])\n",
    "        test_images.append(os.path.join(subdir, file))\n",
    "        \n",
    "print(test_laels[0],test_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing time is :51.721883 seconds\n"
     ]
    }
   ],
   "source": [
    "#test the averaging model on the validation set\n",
    "import time\n",
    "predict=[]\n",
    "length=len(test_images)\n",
    "t1 = time.time()\n",
    "for i in range((length//127)+1):\n",
    "    inputimg=test_images[127*i:127*(i+1)]\n",
    "    test_batch=[]\n",
    "    for path in inputimg:\n",
    "        thisimg=np.array(Image.open(path))/255\n",
    "        test_batch.append(thisimg)\n",
    "    #print(i, np.array(test_batch).shape)\n",
    "    model_batch=model.predict(np.array(test_batch))\n",
    "    predict_batch=list(np.argmax(model_batch,axis=1))\n",
    "    predict_batch=[label[con] for con in predict_batch]\n",
    "    predict.append(predict_batch)\n",
    "\n",
    "predict=sum(predict,[])\n",
    "\n",
    "t2 = time.time()\n",
    "print('The testing time is :%f seconds' % (t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability Averaging accuracy:1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc=accuracy_score(test_laels,predict)\n",
    "print('Probability Averaging accuracy:%s'%acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5052    0    0    0    0]\n",
      " [   0  225    0    0    0]\n",
      " [   0    0  200    0    0]\n",
      " [   0    0    0  197    0]\n",
      " [   0    0    0    0  171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5052\n",
      "           1       1.00      1.00      1.00       225\n",
      "           2       1.00      1.00      1.00       200\n",
      "           3       1.00      1.00      1.00       197\n",
      "           4       1.00      1.00      1.00       171\n",
      "\n",
      "    accuracy                           1.00      5845\n",
      "   macro avg       1.00      1.00      1.00      5845\n",
      "weighted avg       1.00      1.00      1.00      5845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_laels, predict))\n",
    "target_names = ['0', '1','2','3','4']\n",
    "print(classification_report(test_laels, predict, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model,load_model\n",
    "from keras import Input\n",
    "from keras.layers import concatenate,Dense,Flatten,Dropout\n",
    "from keras.preprocessing.image import  ImageDataGenerator\n",
    "import keras.callbacks as kcallbacks\n",
    "import os\n",
    "import math\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1\n",
      "1 block1_conv1\n",
      "2 block1_conv1_bn\n",
      "3 block1_conv1_act\n",
      "4 block1_conv2\n",
      "5 block1_conv2_bn\n",
      "6 block1_conv2_act\n",
      "7 block2_sepconv1\n",
      "8 block2_sepconv1_bn\n",
      "9 block2_sepconv2_act\n",
      "10 block2_sepconv2\n",
      "11 block2_sepconv2_bn\n",
      "12 conv2d_15\n",
      "13 block2_pool\n",
      "14 batch_normalization_1\n",
      "15 add_1\n",
      "16 block3_sepconv1_act\n",
      "17 block3_sepconv1\n",
      "18 block3_sepconv1_bn\n",
      "19 block3_sepconv2_act\n",
      "20 block3_sepconv2\n",
      "21 block3_sepconv2_bn\n",
      "22 conv2d_16\n",
      "23 block3_pool\n",
      "24 batch_normalization_2\n",
      "25 add_2\n",
      "26 block4_sepconv1_act\n",
      "27 block4_sepconv1\n",
      "28 block4_sepconv1_bn\n",
      "29 block4_sepconv2_act\n",
      "30 block4_sepconv2\n",
      "31 block4_sepconv2_bn\n",
      "32 conv2d_17\n",
      "33 block4_pool\n",
      "34 batch_normalization_3\n",
      "35 add_3\n",
      "36 block5_sepconv1_act\n",
      "37 block5_sepconv1\n",
      "38 block5_sepconv1_bn\n",
      "39 block5_sepconv2_act\n",
      "40 block5_sepconv2\n",
      "41 block5_sepconv2_bn\n",
      "42 block5_sepconv3_act\n",
      "43 block5_sepconv3\n",
      "44 block5_sepconv3_bn\n",
      "45 add_4\n",
      "46 block6_sepconv1_act\n",
      "47 block6_sepconv1\n",
      "48 block6_sepconv1_bn\n",
      "49 block6_sepconv2_act\n",
      "50 block6_sepconv2\n",
      "51 block6_sepconv2_bn\n",
      "52 block6_sepconv3_act\n",
      "53 block6_sepconv3\n",
      "54 block6_sepconv3_bn\n",
      "55 add_5\n",
      "56 block7_sepconv1_act\n",
      "57 block7_sepconv1\n",
      "58 block7_sepconv1_bn\n",
      "59 block7_sepconv2_act\n",
      "60 block7_sepconv2\n",
      "61 block7_sepconv2_bn\n",
      "62 block7_sepconv3_act\n",
      "63 block7_sepconv3\n",
      "64 block7_sepconv3_bn\n",
      "65 add_6\n",
      "66 block8_sepconv1_act\n",
      "67 block8_sepconv1\n",
      "68 block8_sepconv1_bn\n",
      "69 block8_sepconv2_act\n",
      "70 block8_sepconv2\n",
      "71 block8_sepconv2_bn\n",
      "72 block8_sepconv3_act\n",
      "73 block8_sepconv3\n",
      "74 block8_sepconv3_bn\n",
      "75 add_7\n",
      "76 block9_sepconv1_act\n",
      "77 block9_sepconv1\n",
      "78 block9_sepconv1_bn\n",
      "79 block9_sepconv2_act\n",
      "80 block9_sepconv2\n",
      "81 block9_sepconv2_bn\n",
      "82 block9_sepconv3_act\n",
      "83 block9_sepconv3\n",
      "84 block9_sepconv3_bn\n",
      "85 add_8\n",
      "86 block10_sepconv1_act\n",
      "87 block10_sepconv1\n",
      "88 block10_sepconv1_bn\n",
      "89 block10_sepconv2_act\n",
      "90 block10_sepconv2\n",
      "91 block10_sepconv2_bn\n",
      "92 block10_sepconv3_act\n",
      "93 block10_sepconv3\n",
      "94 block10_sepconv3_bn\n",
      "95 add_9\n",
      "96 block11_sepconv1_act\n",
      "97 block11_sepconv1\n",
      "98 block11_sepconv1_bn\n",
      "99 block11_sepconv2_act\n",
      "100 block11_sepconv2\n",
      "101 block11_sepconv2_bn\n",
      "102 block11_sepconv3_act\n",
      "103 block11_sepconv3\n",
      "104 block11_sepconv3_bn\n",
      "105 add_10\n",
      "106 block12_sepconv1_act\n",
      "107 block12_sepconv1\n",
      "108 block12_sepconv1_bn\n",
      "109 block12_sepconv2_act\n",
      "110 block12_sepconv2\n",
      "111 block12_sepconv2_bn\n",
      "112 block12_sepconv3_act\n",
      "113 block12_sepconv3\n",
      "114 block12_sepconv3_bn\n",
      "115 add_11\n",
      "116 block13_sepconv1_act\n",
      "117 block13_sepconv1\n",
      "118 block13_sepconv1_bn\n",
      "119 block13_sepconv2_act\n",
      "120 block13_sepconv2\n",
      "121 block13_sepconv2_bn\n",
      "122 conv2d_18\n",
      "123 block13_pool\n",
      "124 batch_normalization_4\n",
      "125 add_12\n",
      "126 block14_sepconv1\n",
      "127 block14_sepconv1_bn\n",
      "128 block14_sepconv1_act\n",
      "129 block14_sepconv2\n",
      "130 block14_sepconv2_bn\n",
      "131 block14_sepconv2_act\n",
      "132 global_average_pooling2d_3\n",
      "133 dense_5\n",
      "134 dropout_3\n",
      "135 dense_6\n"
     ]
    }
   ],
   "source": [
    "for i,layer in enumerate(xception_model.layers):\n",
    "    print(i,layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_2\n",
      "1 block1_conv1\n",
      "2 block1_conv2\n",
      "3 block1_pool\n",
      "4 block2_conv1\n",
      "5 block2_conv2\n",
      "6 block2_pool\n",
      "7 block3_conv1\n",
      "8 block3_conv2\n",
      "9 block3_conv3\n",
      "10 block3_pool\n",
      "11 block4_conv1\n",
      "12 block4_conv2\n",
      "13 block4_conv3\n",
      "14 block4_pool\n",
      "15 block5_conv1\n",
      "16 block5_conv2\n",
      "17 block5_conv3\n",
      "18 block5_pool\n",
      "19 global_average_pooling2d_4\n",
      "20 dense_7\n",
      "21 dropout_4\n",
      "22 dense_8\n"
     ]
    }
   ],
   "source": [
    "for i,layer in enumerate(vgg_model.layers):\n",
    "    print(i,layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_3\n",
      "1 block1_conv1\n",
      "2 block1_conv2\n",
      "3 block1_pool\n",
      "4 block2_conv1\n",
      "5 block2_conv2\n",
      "6 block2_pool\n",
      "7 block3_conv1\n",
      "8 block3_conv2\n",
      "9 block3_conv3\n",
      "10 block3_conv4\n",
      "11 block3_pool\n",
      "12 block4_conv1\n",
      "13 block4_conv2\n",
      "14 block4_conv3\n",
      "15 block4_conv4\n",
      "16 block4_pool\n",
      "17 block5_conv1\n",
      "18 block5_conv2\n",
      "19 block5_conv3\n",
      "20 block5_conv4\n",
      "21 block5_pool\n",
      "22 global_average_pooling2d_5\n",
      "23 dense_9\n",
      "24 dropout_5\n",
      "25 dense_10\n"
     ]
    }
   ],
   "source": [
    "for i,layer in enumerate(vgg19_model.layers):\n",
    "    print(i,layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_5\n",
      "1 conv2d_19\n",
      "2 batch_normalization_5\n",
      "3 activation_50\n",
      "4 conv2d_20\n",
      "5 batch_normalization_6\n",
      "6 activation_51\n",
      "7 conv2d_21\n",
      "8 batch_normalization_7\n",
      "9 activation_52\n",
      "10 max_pooling2d_6\n",
      "11 conv2d_22\n",
      "12 batch_normalization_8\n",
      "13 activation_53\n",
      "14 conv2d_23\n",
      "15 batch_normalization_9\n",
      "16 activation_54\n",
      "17 max_pooling2d_7\n",
      "18 conv2d_27\n",
      "19 batch_normalization_13\n",
      "20 activation_58\n",
      "21 conv2d_25\n",
      "22 conv2d_28\n",
      "23 batch_normalization_11\n",
      "24 batch_normalization_14\n",
      "25 activation_56\n",
      "26 activation_59\n",
      "27 average_pooling2d_1\n",
      "28 conv2d_24\n",
      "29 conv2d_26\n",
      "30 conv2d_29\n",
      "31 conv2d_30\n",
      "32 batch_normalization_10\n",
      "33 batch_normalization_12\n",
      "34 batch_normalization_15\n",
      "35 batch_normalization_16\n",
      "36 activation_55\n",
      "37 activation_57\n",
      "38 activation_60\n",
      "39 activation_61\n",
      "40 mixed0\n",
      "41 conv2d_34\n",
      "42 batch_normalization_20\n",
      "43 activation_65\n",
      "44 conv2d_32\n",
      "45 conv2d_35\n",
      "46 batch_normalization_18\n",
      "47 batch_normalization_21\n",
      "48 activation_63\n",
      "49 activation_66\n",
      "50 average_pooling2d_2\n",
      "51 conv2d_31\n",
      "52 conv2d_33\n",
      "53 conv2d_36\n",
      "54 conv2d_37\n",
      "55 batch_normalization_17\n",
      "56 batch_normalization_19\n",
      "57 batch_normalization_22\n",
      "58 batch_normalization_23\n",
      "59 activation_62\n",
      "60 activation_64\n",
      "61 activation_67\n",
      "62 activation_68\n",
      "63 mixed1\n",
      "64 conv2d_41\n",
      "65 batch_normalization_27\n",
      "66 activation_72\n",
      "67 conv2d_39\n",
      "68 conv2d_42\n",
      "69 batch_normalization_25\n",
      "70 batch_normalization_28\n",
      "71 activation_70\n",
      "72 activation_73\n",
      "73 average_pooling2d_3\n",
      "74 conv2d_38\n",
      "75 conv2d_40\n",
      "76 conv2d_43\n",
      "77 conv2d_44\n",
      "78 batch_normalization_24\n",
      "79 batch_normalization_26\n",
      "80 batch_normalization_29\n",
      "81 batch_normalization_30\n",
      "82 activation_69\n",
      "83 activation_71\n",
      "84 activation_74\n",
      "85 activation_75\n",
      "86 mixed2\n",
      "87 conv2d_46\n",
      "88 batch_normalization_32\n",
      "89 activation_77\n",
      "90 conv2d_47\n",
      "91 batch_normalization_33\n",
      "92 activation_78\n",
      "93 conv2d_45\n",
      "94 conv2d_48\n",
      "95 batch_normalization_31\n",
      "96 batch_normalization_34\n",
      "97 activation_76\n",
      "98 activation_79\n",
      "99 max_pooling2d_8\n",
      "100 mixed3\n",
      "101 conv2d_53\n",
      "102 batch_normalization_39\n",
      "103 activation_84\n",
      "104 conv2d_54\n",
      "105 batch_normalization_40\n",
      "106 activation_85\n",
      "107 conv2d_50\n",
      "108 conv2d_55\n",
      "109 batch_normalization_36\n",
      "110 batch_normalization_41\n",
      "111 activation_81\n",
      "112 activation_86\n",
      "113 conv2d_51\n",
      "114 conv2d_56\n",
      "115 batch_normalization_37\n",
      "116 batch_normalization_42\n",
      "117 activation_82\n",
      "118 activation_87\n",
      "119 average_pooling2d_4\n",
      "120 conv2d_49\n",
      "121 conv2d_52\n",
      "122 conv2d_57\n",
      "123 conv2d_58\n",
      "124 batch_normalization_35\n",
      "125 batch_normalization_38\n",
      "126 batch_normalization_43\n",
      "127 batch_normalization_44\n",
      "128 activation_80\n",
      "129 activation_83\n",
      "130 activation_88\n",
      "131 activation_89\n",
      "132 mixed4\n",
      "133 conv2d_63\n",
      "134 batch_normalization_49\n",
      "135 activation_94\n",
      "136 conv2d_64\n",
      "137 batch_normalization_50\n",
      "138 activation_95\n",
      "139 conv2d_60\n",
      "140 conv2d_65\n",
      "141 batch_normalization_46\n",
      "142 batch_normalization_51\n",
      "143 activation_91\n",
      "144 activation_96\n",
      "145 conv2d_61\n",
      "146 conv2d_66\n",
      "147 batch_normalization_47\n",
      "148 batch_normalization_52\n",
      "149 activation_92\n",
      "150 activation_97\n",
      "151 average_pooling2d_5\n",
      "152 conv2d_59\n",
      "153 conv2d_62\n",
      "154 conv2d_67\n",
      "155 conv2d_68\n",
      "156 batch_normalization_45\n",
      "157 batch_normalization_48\n",
      "158 batch_normalization_53\n",
      "159 batch_normalization_54\n",
      "160 activation_90\n",
      "161 activation_93\n",
      "162 activation_98\n",
      "163 activation_99\n",
      "164 mixed5\n",
      "165 conv2d_73\n",
      "166 batch_normalization_59\n",
      "167 activation_104\n",
      "168 conv2d_74\n",
      "169 batch_normalization_60\n",
      "170 activation_105\n",
      "171 conv2d_70\n",
      "172 conv2d_75\n",
      "173 batch_normalization_56\n",
      "174 batch_normalization_61\n",
      "175 activation_101\n",
      "176 activation_106\n",
      "177 conv2d_71\n",
      "178 conv2d_76\n",
      "179 batch_normalization_57\n",
      "180 batch_normalization_62\n",
      "181 activation_102\n",
      "182 activation_107\n",
      "183 average_pooling2d_6\n",
      "184 conv2d_69\n",
      "185 conv2d_72\n",
      "186 conv2d_77\n",
      "187 conv2d_78\n",
      "188 batch_normalization_55\n",
      "189 batch_normalization_58\n",
      "190 batch_normalization_63\n",
      "191 batch_normalization_64\n",
      "192 activation_100\n",
      "193 activation_103\n",
      "194 activation_108\n",
      "195 activation_109\n",
      "196 mixed6\n",
      "197 conv2d_83\n",
      "198 batch_normalization_69\n",
      "199 activation_114\n",
      "200 conv2d_84\n",
      "201 batch_normalization_70\n",
      "202 activation_115\n",
      "203 conv2d_80\n",
      "204 conv2d_85\n",
      "205 batch_normalization_66\n",
      "206 batch_normalization_71\n",
      "207 activation_111\n",
      "208 activation_116\n",
      "209 conv2d_81\n",
      "210 conv2d_86\n",
      "211 batch_normalization_67\n",
      "212 batch_normalization_72\n",
      "213 activation_112\n",
      "214 activation_117\n",
      "215 average_pooling2d_7\n",
      "216 conv2d_79\n",
      "217 conv2d_82\n",
      "218 conv2d_87\n",
      "219 conv2d_88\n",
      "220 batch_normalization_65\n",
      "221 batch_normalization_68\n",
      "222 batch_normalization_73\n",
      "223 batch_normalization_74\n",
      "224 activation_110\n",
      "225 activation_113\n",
      "226 activation_118\n",
      "227 activation_119\n",
      "228 mixed7\n",
      "229 conv2d_91\n",
      "230 batch_normalization_77\n",
      "231 activation_122\n",
      "232 conv2d_92\n",
      "233 batch_normalization_78\n",
      "234 activation_123\n",
      "235 conv2d_89\n",
      "236 conv2d_93\n",
      "237 batch_normalization_75\n",
      "238 batch_normalization_79\n",
      "239 activation_120\n",
      "240 activation_124\n",
      "241 conv2d_90\n",
      "242 conv2d_94\n",
      "243 batch_normalization_76\n",
      "244 batch_normalization_80\n",
      "245 activation_121\n",
      "246 activation_125\n",
      "247 max_pooling2d_9\n",
      "248 mixed8\n",
      "249 conv2d_99\n",
      "250 batch_normalization_85\n",
      "251 activation_130\n",
      "252 conv2d_96\n",
      "253 conv2d_100\n",
      "254 batch_normalization_82\n",
      "255 batch_normalization_86\n",
      "256 activation_127\n",
      "257 activation_131\n",
      "258 conv2d_97\n",
      "259 conv2d_98\n",
      "260 conv2d_101\n",
      "261 conv2d_102\n",
      "262 average_pooling2d_8\n",
      "263 conv2d_95\n",
      "264 batch_normalization_83\n",
      "265 batch_normalization_84\n",
      "266 batch_normalization_87\n",
      "267 batch_normalization_88\n",
      "268 conv2d_103\n",
      "269 batch_normalization_81\n",
      "270 activation_128\n",
      "271 activation_129\n",
      "272 activation_132\n",
      "273 activation_133\n",
      "274 batch_normalization_89\n",
      "275 activation_126\n",
      "276 mixed9_0\n",
      "277 concatenate_1\n",
      "278 activation_134\n",
      "279 mixed9\n",
      "280 conv2d_108\n",
      "281 batch_normalization_94\n",
      "282 activation_139\n",
      "283 conv2d_105\n",
      "284 conv2d_109\n",
      "285 batch_normalization_91\n",
      "286 batch_normalization_95\n",
      "287 activation_136\n",
      "288 activation_140\n",
      "289 conv2d_106\n",
      "290 conv2d_107\n",
      "291 conv2d_110\n",
      "292 conv2d_111\n",
      "293 average_pooling2d_9\n",
      "294 conv2d_104\n",
      "295 batch_normalization_92\n",
      "296 batch_normalization_93\n",
      "297 batch_normalization_96\n",
      "298 batch_normalization_97\n",
      "299 conv2d_112\n",
      "300 batch_normalization_90\n",
      "301 activation_137\n",
      "302 activation_138\n",
      "303 activation_141\n",
      "304 activation_142\n",
      "305 batch_normalization_98\n",
      "306 activation_135\n",
      "307 mixed9_1\n",
      "308 concatenate_2\n",
      "309 activation_143\n",
      "310 mixed10\n",
      "311 global_average_pooling2d_7\n",
      "312 dense_13\n",
      "313 dropout_7\n",
      "314 dense_14\n"
     ]
    }
   ],
   "source": [
    "for i,layer in enumerate(incep_model.layers):\n",
    "    print(i,layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_6\n",
      "1 conv2d_113\n",
      "2 batch_normalization_99\n",
      "3 activation_144\n",
      "4 conv2d_114\n",
      "5 batch_normalization_100\n",
      "6 activation_145\n",
      "7 conv2d_115\n",
      "8 batch_normalization_101\n",
      "9 activation_146\n",
      "10 max_pooling2d_10\n",
      "11 conv2d_116\n",
      "12 batch_normalization_102\n",
      "13 activation_147\n",
      "14 conv2d_117\n",
      "15 batch_normalization_103\n",
      "16 activation_148\n",
      "17 max_pooling2d_11\n",
      "18 conv2d_121\n",
      "19 batch_normalization_107\n",
      "20 activation_152\n",
      "21 conv2d_119\n",
      "22 conv2d_122\n",
      "23 batch_normalization_105\n",
      "24 batch_normalization_108\n",
      "25 activation_150\n",
      "26 activation_153\n",
      "27 average_pooling2d_10\n",
      "28 conv2d_118\n",
      "29 conv2d_120\n",
      "30 conv2d_123\n",
      "31 conv2d_124\n",
      "32 batch_normalization_104\n",
      "33 batch_normalization_106\n",
      "34 batch_normalization_109\n",
      "35 batch_normalization_110\n",
      "36 activation_149\n",
      "37 activation_151\n",
      "38 activation_154\n",
      "39 activation_155\n",
      "40 mixed_5b\n",
      "41 conv2d_128\n",
      "42 batch_normalization_114\n",
      "43 activation_159\n",
      "44 conv2d_126\n",
      "45 conv2d_129\n",
      "46 batch_normalization_112\n",
      "47 batch_normalization_115\n",
      "48 activation_157\n",
      "49 activation_160\n",
      "50 conv2d_125\n",
      "51 conv2d_127\n",
      "52 conv2d_130\n",
      "53 batch_normalization_111\n",
      "54 batch_normalization_113\n",
      "55 batch_normalization_116\n",
      "56 activation_156\n",
      "57 activation_158\n",
      "58 activation_161\n",
      "59 block35_1_mixed\n",
      "60 block35_1_conv\n",
      "61 block35_1\n",
      "62 block35_1_ac\n",
      "63 conv2d_134\n",
      "64 batch_normalization_120\n",
      "65 activation_165\n",
      "66 conv2d_132\n",
      "67 conv2d_135\n",
      "68 batch_normalization_118\n",
      "69 batch_normalization_121\n",
      "70 activation_163\n",
      "71 activation_166\n",
      "72 conv2d_131\n",
      "73 conv2d_133\n",
      "74 conv2d_136\n",
      "75 batch_normalization_117\n",
      "76 batch_normalization_119\n",
      "77 batch_normalization_122\n",
      "78 activation_162\n",
      "79 activation_164\n",
      "80 activation_167\n",
      "81 block35_2_mixed\n",
      "82 block35_2_conv\n",
      "83 block35_2\n",
      "84 block35_2_ac\n",
      "85 conv2d_140\n",
      "86 batch_normalization_126\n",
      "87 activation_171\n",
      "88 conv2d_138\n",
      "89 conv2d_141\n",
      "90 batch_normalization_124\n",
      "91 batch_normalization_127\n",
      "92 activation_169\n",
      "93 activation_172\n",
      "94 conv2d_137\n",
      "95 conv2d_139\n",
      "96 conv2d_142\n",
      "97 batch_normalization_123\n",
      "98 batch_normalization_125\n",
      "99 batch_normalization_128\n",
      "100 activation_168\n",
      "101 activation_170\n",
      "102 activation_173\n",
      "103 block35_3_mixed\n",
      "104 block35_3_conv\n",
      "105 block35_3\n",
      "106 block35_3_ac\n",
      "107 conv2d_146\n",
      "108 batch_normalization_132\n",
      "109 activation_177\n",
      "110 conv2d_144\n",
      "111 conv2d_147\n",
      "112 batch_normalization_130\n",
      "113 batch_normalization_133\n",
      "114 activation_175\n",
      "115 activation_178\n",
      "116 conv2d_143\n",
      "117 conv2d_145\n",
      "118 conv2d_148\n",
      "119 batch_normalization_129\n",
      "120 batch_normalization_131\n",
      "121 batch_normalization_134\n",
      "122 activation_174\n",
      "123 activation_176\n",
      "124 activation_179\n",
      "125 block35_4_mixed\n",
      "126 block35_4_conv\n",
      "127 block35_4\n",
      "128 block35_4_ac\n",
      "129 conv2d_152\n",
      "130 batch_normalization_138\n",
      "131 activation_183\n",
      "132 conv2d_150\n",
      "133 conv2d_153\n",
      "134 batch_normalization_136\n",
      "135 batch_normalization_139\n",
      "136 activation_181\n",
      "137 activation_184\n",
      "138 conv2d_149\n",
      "139 conv2d_151\n",
      "140 conv2d_154\n",
      "141 batch_normalization_135\n",
      "142 batch_normalization_137\n",
      "143 batch_normalization_140\n",
      "144 activation_180\n",
      "145 activation_182\n",
      "146 activation_185\n",
      "147 block35_5_mixed\n",
      "148 block35_5_conv\n",
      "149 block35_5\n",
      "150 block35_5_ac\n",
      "151 conv2d_158\n",
      "152 batch_normalization_144\n",
      "153 activation_189\n",
      "154 conv2d_156\n",
      "155 conv2d_159\n",
      "156 batch_normalization_142\n",
      "157 batch_normalization_145\n",
      "158 activation_187\n",
      "159 activation_190\n",
      "160 conv2d_155\n",
      "161 conv2d_157\n",
      "162 conv2d_160\n",
      "163 batch_normalization_141\n",
      "164 batch_normalization_143\n",
      "165 batch_normalization_146\n",
      "166 activation_186\n",
      "167 activation_188\n",
      "168 activation_191\n",
      "169 block35_6_mixed\n",
      "170 block35_6_conv\n",
      "171 block35_6\n",
      "172 block35_6_ac\n",
      "173 conv2d_164\n",
      "174 batch_normalization_150\n",
      "175 activation_195\n",
      "176 conv2d_162\n",
      "177 conv2d_165\n",
      "178 batch_normalization_148\n",
      "179 batch_normalization_151\n",
      "180 activation_193\n",
      "181 activation_196\n",
      "182 conv2d_161\n",
      "183 conv2d_163\n",
      "184 conv2d_166\n",
      "185 batch_normalization_147\n",
      "186 batch_normalization_149\n",
      "187 batch_normalization_152\n",
      "188 activation_192\n",
      "189 activation_194\n",
      "190 activation_197\n",
      "191 block35_7_mixed\n",
      "192 block35_7_conv\n",
      "193 block35_7\n",
      "194 block35_7_ac\n",
      "195 conv2d_170\n",
      "196 batch_normalization_156\n",
      "197 activation_201\n",
      "198 conv2d_168\n",
      "199 conv2d_171\n",
      "200 batch_normalization_154\n",
      "201 batch_normalization_157\n",
      "202 activation_199\n",
      "203 activation_202\n",
      "204 conv2d_167\n",
      "205 conv2d_169\n",
      "206 conv2d_172\n",
      "207 batch_normalization_153\n",
      "208 batch_normalization_155\n",
      "209 batch_normalization_158\n",
      "210 activation_198\n",
      "211 activation_200\n",
      "212 activation_203\n",
      "213 block35_8_mixed\n",
      "214 block35_8_conv\n",
      "215 block35_8\n",
      "216 block35_8_ac\n",
      "217 conv2d_176\n",
      "218 batch_normalization_162\n",
      "219 activation_207\n",
      "220 conv2d_174\n",
      "221 conv2d_177\n",
      "222 batch_normalization_160\n",
      "223 batch_normalization_163\n",
      "224 activation_205\n",
      "225 activation_208\n",
      "226 conv2d_173\n",
      "227 conv2d_175\n",
      "228 conv2d_178\n",
      "229 batch_normalization_159\n",
      "230 batch_normalization_161\n",
      "231 batch_normalization_164\n",
      "232 activation_204\n",
      "233 activation_206\n",
      "234 activation_209\n",
      "235 block35_9_mixed\n",
      "236 block35_9_conv\n",
      "237 block35_9\n",
      "238 block35_9_ac\n",
      "239 conv2d_182\n",
      "240 batch_normalization_168\n",
      "241 activation_213\n",
      "242 conv2d_180\n",
      "243 conv2d_183\n",
      "244 batch_normalization_166\n",
      "245 batch_normalization_169\n",
      "246 activation_211\n",
      "247 activation_214\n",
      "248 conv2d_179\n",
      "249 conv2d_181\n",
      "250 conv2d_184\n",
      "251 batch_normalization_165\n",
      "252 batch_normalization_167\n",
      "253 batch_normalization_170\n",
      "254 activation_210\n",
      "255 activation_212\n",
      "256 activation_215\n",
      "257 block35_10_mixed\n",
      "258 block35_10_conv\n",
      "259 block35_10\n",
      "260 block35_10_ac\n",
      "261 conv2d_186\n",
      "262 batch_normalization_172\n",
      "263 activation_217\n",
      "264 conv2d_187\n",
      "265 batch_normalization_173\n",
      "266 activation_218\n",
      "267 conv2d_185\n",
      "268 conv2d_188\n",
      "269 batch_normalization_171\n",
      "270 batch_normalization_174\n",
      "271 activation_216\n",
      "272 activation_219\n",
      "273 max_pooling2d_12\n",
      "274 mixed_6a\n",
      "275 conv2d_190\n",
      "276 batch_normalization_176\n",
      "277 activation_221\n",
      "278 conv2d_191\n",
      "279 batch_normalization_177\n",
      "280 activation_222\n",
      "281 conv2d_189\n",
      "282 conv2d_192\n",
      "283 batch_normalization_175\n",
      "284 batch_normalization_178\n",
      "285 activation_220\n",
      "286 activation_223\n",
      "287 block17_1_mixed\n",
      "288 block17_1_conv\n",
      "289 block17_1\n",
      "290 block17_1_ac\n",
      "291 conv2d_194\n",
      "292 batch_normalization_180\n",
      "293 activation_225\n",
      "294 conv2d_195\n",
      "295 batch_normalization_181\n",
      "296 activation_226\n",
      "297 conv2d_193\n",
      "298 conv2d_196\n",
      "299 batch_normalization_179\n",
      "300 batch_normalization_182\n",
      "301 activation_224\n",
      "302 activation_227\n",
      "303 block17_2_mixed\n",
      "304 block17_2_conv\n",
      "305 block17_2\n",
      "306 block17_2_ac\n",
      "307 conv2d_198\n",
      "308 batch_normalization_184\n",
      "309 activation_229\n",
      "310 conv2d_199\n",
      "311 batch_normalization_185\n",
      "312 activation_230\n",
      "313 conv2d_197\n",
      "314 conv2d_200\n",
      "315 batch_normalization_183\n",
      "316 batch_normalization_186\n",
      "317 activation_228\n",
      "318 activation_231\n",
      "319 block17_3_mixed\n",
      "320 block17_3_conv\n",
      "321 block17_3\n",
      "322 block17_3_ac\n",
      "323 conv2d_202\n",
      "324 batch_normalization_188\n",
      "325 activation_233\n",
      "326 conv2d_203\n",
      "327 batch_normalization_189\n",
      "328 activation_234\n",
      "329 conv2d_201\n",
      "330 conv2d_204\n",
      "331 batch_normalization_187\n",
      "332 batch_normalization_190\n",
      "333 activation_232\n",
      "334 activation_235\n",
      "335 block17_4_mixed\n",
      "336 block17_4_conv\n",
      "337 block17_4\n",
      "338 block17_4_ac\n",
      "339 conv2d_206\n",
      "340 batch_normalization_192\n",
      "341 activation_237\n",
      "342 conv2d_207\n",
      "343 batch_normalization_193\n",
      "344 activation_238\n",
      "345 conv2d_205\n",
      "346 conv2d_208\n",
      "347 batch_normalization_191\n",
      "348 batch_normalization_194\n",
      "349 activation_236\n",
      "350 activation_239\n",
      "351 block17_5_mixed\n",
      "352 block17_5_conv\n",
      "353 block17_5\n",
      "354 block17_5_ac\n",
      "355 conv2d_210\n",
      "356 batch_normalization_196\n",
      "357 activation_241\n",
      "358 conv2d_211\n",
      "359 batch_normalization_197\n",
      "360 activation_242\n",
      "361 conv2d_209\n",
      "362 conv2d_212\n",
      "363 batch_normalization_195\n",
      "364 batch_normalization_198\n",
      "365 activation_240\n",
      "366 activation_243\n",
      "367 block17_6_mixed\n",
      "368 block17_6_conv\n",
      "369 block17_6\n",
      "370 block17_6_ac\n",
      "371 conv2d_214\n",
      "372 batch_normalization_200\n",
      "373 activation_245\n",
      "374 conv2d_215\n",
      "375 batch_normalization_201\n",
      "376 activation_246\n",
      "377 conv2d_213\n",
      "378 conv2d_216\n",
      "379 batch_normalization_199\n",
      "380 batch_normalization_202\n",
      "381 activation_244\n",
      "382 activation_247\n",
      "383 block17_7_mixed\n",
      "384 block17_7_conv\n",
      "385 block17_7\n",
      "386 block17_7_ac\n",
      "387 conv2d_218\n",
      "388 batch_normalization_204\n",
      "389 activation_249\n",
      "390 conv2d_219\n",
      "391 batch_normalization_205\n",
      "392 activation_250\n",
      "393 conv2d_217\n",
      "394 conv2d_220\n",
      "395 batch_normalization_203\n",
      "396 batch_normalization_206\n",
      "397 activation_248\n",
      "398 activation_251\n",
      "399 block17_8_mixed\n",
      "400 block17_8_conv\n",
      "401 block17_8\n",
      "402 block17_8_ac\n",
      "403 conv2d_222\n",
      "404 batch_normalization_208\n",
      "405 activation_253\n",
      "406 conv2d_223\n",
      "407 batch_normalization_209\n",
      "408 activation_254\n",
      "409 conv2d_221\n",
      "410 conv2d_224\n",
      "411 batch_normalization_207\n",
      "412 batch_normalization_210\n",
      "413 activation_252\n",
      "414 activation_255\n",
      "415 block17_9_mixed\n",
      "416 block17_9_conv\n",
      "417 block17_9\n",
      "418 block17_9_ac\n",
      "419 conv2d_226\n",
      "420 batch_normalization_212\n",
      "421 activation_257\n",
      "422 conv2d_227\n",
      "423 batch_normalization_213\n",
      "424 activation_258\n",
      "425 conv2d_225\n",
      "426 conv2d_228\n",
      "427 batch_normalization_211\n",
      "428 batch_normalization_214\n",
      "429 activation_256\n",
      "430 activation_259\n",
      "431 block17_10_mixed\n",
      "432 block17_10_conv\n",
      "433 block17_10\n",
      "434 block17_10_ac\n",
      "435 conv2d_230\n",
      "436 batch_normalization_216\n",
      "437 activation_261\n",
      "438 conv2d_231\n",
      "439 batch_normalization_217\n",
      "440 activation_262\n",
      "441 conv2d_229\n",
      "442 conv2d_232\n",
      "443 batch_normalization_215\n",
      "444 batch_normalization_218\n",
      "445 activation_260\n",
      "446 activation_263\n",
      "447 block17_11_mixed\n",
      "448 block17_11_conv\n",
      "449 block17_11\n",
      "450 block17_11_ac\n",
      "451 conv2d_234\n",
      "452 batch_normalization_220\n",
      "453 activation_265\n",
      "454 conv2d_235\n",
      "455 batch_normalization_221\n",
      "456 activation_266\n",
      "457 conv2d_233\n",
      "458 conv2d_236\n",
      "459 batch_normalization_219\n",
      "460 batch_normalization_222\n",
      "461 activation_264\n",
      "462 activation_267\n",
      "463 block17_12_mixed\n",
      "464 block17_12_conv\n",
      "465 block17_12\n",
      "466 block17_12_ac\n",
      "467 conv2d_238\n",
      "468 batch_normalization_224\n",
      "469 activation_269\n",
      "470 conv2d_239\n",
      "471 batch_normalization_225\n",
      "472 activation_270\n",
      "473 conv2d_237\n",
      "474 conv2d_240\n",
      "475 batch_normalization_223\n",
      "476 batch_normalization_226\n",
      "477 activation_268\n",
      "478 activation_271\n",
      "479 block17_13_mixed\n",
      "480 block17_13_conv\n",
      "481 block17_13\n",
      "482 block17_13_ac\n",
      "483 conv2d_242\n",
      "484 batch_normalization_228\n",
      "485 activation_273\n",
      "486 conv2d_243\n",
      "487 batch_normalization_229\n",
      "488 activation_274\n",
      "489 conv2d_241\n",
      "490 conv2d_244\n",
      "491 batch_normalization_227\n",
      "492 batch_normalization_230\n",
      "493 activation_272\n",
      "494 activation_275\n",
      "495 block17_14_mixed\n",
      "496 block17_14_conv\n",
      "497 block17_14\n",
      "498 block17_14_ac\n",
      "499 conv2d_246\n",
      "500 batch_normalization_232\n",
      "501 activation_277\n",
      "502 conv2d_247\n",
      "503 batch_normalization_233\n",
      "504 activation_278\n",
      "505 conv2d_245\n",
      "506 conv2d_248\n",
      "507 batch_normalization_231\n",
      "508 batch_normalization_234\n",
      "509 activation_276\n",
      "510 activation_279\n",
      "511 block17_15_mixed\n",
      "512 block17_15_conv\n",
      "513 block17_15\n",
      "514 block17_15_ac\n",
      "515 conv2d_250\n",
      "516 batch_normalization_236\n",
      "517 activation_281\n",
      "518 conv2d_251\n",
      "519 batch_normalization_237\n",
      "520 activation_282\n",
      "521 conv2d_249\n",
      "522 conv2d_252\n",
      "523 batch_normalization_235\n",
      "524 batch_normalization_238\n",
      "525 activation_280\n",
      "526 activation_283\n",
      "527 block17_16_mixed\n",
      "528 block17_16_conv\n",
      "529 block17_16\n",
      "530 block17_16_ac\n",
      "531 conv2d_254\n",
      "532 batch_normalization_240\n",
      "533 activation_285\n",
      "534 conv2d_255\n",
      "535 batch_normalization_241\n",
      "536 activation_286\n",
      "537 conv2d_253\n",
      "538 conv2d_256\n",
      "539 batch_normalization_239\n",
      "540 batch_normalization_242\n",
      "541 activation_284\n",
      "542 activation_287\n",
      "543 block17_17_mixed\n",
      "544 block17_17_conv\n",
      "545 block17_17\n",
      "546 block17_17_ac\n",
      "547 conv2d_258\n",
      "548 batch_normalization_244\n",
      "549 activation_289\n",
      "550 conv2d_259\n",
      "551 batch_normalization_245\n",
      "552 activation_290\n",
      "553 conv2d_257\n",
      "554 conv2d_260\n",
      "555 batch_normalization_243\n",
      "556 batch_normalization_246\n",
      "557 activation_288\n",
      "558 activation_291\n",
      "559 block17_18_mixed\n",
      "560 block17_18_conv\n",
      "561 block17_18\n",
      "562 block17_18_ac\n",
      "563 conv2d_262\n",
      "564 batch_normalization_248\n",
      "565 activation_293\n",
      "566 conv2d_263\n",
      "567 batch_normalization_249\n",
      "568 activation_294\n",
      "569 conv2d_261\n",
      "570 conv2d_264\n",
      "571 batch_normalization_247\n",
      "572 batch_normalization_250\n",
      "573 activation_292\n",
      "574 activation_295\n",
      "575 block17_19_mixed\n",
      "576 block17_19_conv\n",
      "577 block17_19\n",
      "578 block17_19_ac\n",
      "579 conv2d_266\n",
      "580 batch_normalization_252\n",
      "581 activation_297\n",
      "582 conv2d_267\n",
      "583 batch_normalization_253\n",
      "584 activation_298\n",
      "585 conv2d_265\n",
      "586 conv2d_268\n",
      "587 batch_normalization_251\n",
      "588 batch_normalization_254\n",
      "589 activation_296\n",
      "590 activation_299\n",
      "591 block17_20_mixed\n",
      "592 block17_20_conv\n",
      "593 block17_20\n",
      "594 block17_20_ac\n",
      "595 conv2d_273\n",
      "596 batch_normalization_259\n",
      "597 activation_304\n",
      "598 conv2d_269\n",
      "599 conv2d_271\n",
      "600 conv2d_274\n",
      "601 batch_normalization_255\n",
      "602 batch_normalization_257\n",
      "603 batch_normalization_260\n",
      "604 activation_300\n",
      "605 activation_302\n",
      "606 activation_305\n",
      "607 conv2d_270\n",
      "608 conv2d_272\n",
      "609 conv2d_275\n",
      "610 batch_normalization_256\n",
      "611 batch_normalization_258\n",
      "612 batch_normalization_261\n",
      "613 activation_301\n",
      "614 activation_303\n",
      "615 activation_306\n",
      "616 max_pooling2d_13\n",
      "617 mixed_7a\n",
      "618 conv2d_277\n",
      "619 batch_normalization_263\n",
      "620 activation_308\n",
      "621 conv2d_278\n",
      "622 batch_normalization_264\n",
      "623 activation_309\n",
      "624 conv2d_276\n",
      "625 conv2d_279\n",
      "626 batch_normalization_262\n",
      "627 batch_normalization_265\n",
      "628 activation_307\n",
      "629 activation_310\n",
      "630 block8_1_mixed\n",
      "631 block8_1_conv\n",
      "632 block8_1\n",
      "633 block8_1_ac\n",
      "634 conv2d_281\n",
      "635 batch_normalization_267\n",
      "636 activation_312\n",
      "637 conv2d_282\n",
      "638 batch_normalization_268\n",
      "639 activation_313\n",
      "640 conv2d_280\n",
      "641 conv2d_283\n",
      "642 batch_normalization_266\n",
      "643 batch_normalization_269\n",
      "644 activation_311\n",
      "645 activation_314\n",
      "646 block8_2_mixed\n",
      "647 block8_2_conv\n",
      "648 block8_2\n",
      "649 block8_2_ac\n",
      "650 conv2d_285\n",
      "651 batch_normalization_271\n",
      "652 activation_316\n",
      "653 conv2d_286\n",
      "654 batch_normalization_272\n",
      "655 activation_317\n",
      "656 conv2d_284\n",
      "657 conv2d_287\n",
      "658 batch_normalization_270\n",
      "659 batch_normalization_273\n",
      "660 activation_315\n",
      "661 activation_318\n",
      "662 block8_3_mixed\n",
      "663 block8_3_conv\n",
      "664 block8_3\n",
      "665 block8_3_ac\n",
      "666 conv2d_289\n",
      "667 batch_normalization_275\n",
      "668 activation_320\n",
      "669 conv2d_290\n",
      "670 batch_normalization_276\n",
      "671 activation_321\n",
      "672 conv2d_288\n",
      "673 conv2d_291\n",
      "674 batch_normalization_274\n",
      "675 batch_normalization_277\n",
      "676 activation_319\n",
      "677 activation_322\n",
      "678 block8_4_mixed\n",
      "679 block8_4_conv\n",
      "680 block8_4\n",
      "681 block8_4_ac\n",
      "682 conv2d_293\n",
      "683 batch_normalization_279\n",
      "684 activation_324\n",
      "685 conv2d_294\n",
      "686 batch_normalization_280\n",
      "687 activation_325\n",
      "688 conv2d_292\n",
      "689 conv2d_295\n",
      "690 batch_normalization_278\n",
      "691 batch_normalization_281\n",
      "692 activation_323\n",
      "693 activation_326\n",
      "694 block8_5_mixed\n",
      "695 block8_5_conv\n",
      "696 block8_5\n",
      "697 block8_5_ac\n",
      "698 conv2d_297\n",
      "699 batch_normalization_283\n",
      "700 activation_328\n",
      "701 conv2d_298\n",
      "702 batch_normalization_284\n",
      "703 activation_329\n",
      "704 conv2d_296\n",
      "705 conv2d_299\n",
      "706 batch_normalization_282\n",
      "707 batch_normalization_285\n",
      "708 activation_327\n",
      "709 activation_330\n",
      "710 block8_6_mixed\n",
      "711 block8_6_conv\n",
      "712 block8_6\n",
      "713 block8_6_ac\n",
      "714 conv2d_301\n",
      "715 batch_normalization_287\n",
      "716 activation_332\n",
      "717 conv2d_302\n",
      "718 batch_normalization_288\n",
      "719 activation_333\n",
      "720 conv2d_300\n",
      "721 conv2d_303\n",
      "722 batch_normalization_286\n",
      "723 batch_normalization_289\n",
      "724 activation_331\n",
      "725 activation_334\n",
      "726 block8_7_mixed\n",
      "727 block8_7_conv\n",
      "728 block8_7\n",
      "729 block8_7_ac\n",
      "730 conv2d_305\n",
      "731 batch_normalization_291\n",
      "732 activation_336\n",
      "733 conv2d_306\n",
      "734 batch_normalization_292\n",
      "735 activation_337\n",
      "736 conv2d_304\n",
      "737 conv2d_307\n",
      "738 batch_normalization_290\n",
      "739 batch_normalization_293\n",
      "740 activation_335\n",
      "741 activation_338\n",
      "742 block8_8_mixed\n",
      "743 block8_8_conv\n",
      "744 block8_8\n",
      "745 block8_8_ac\n",
      "746 conv2d_309\n",
      "747 batch_normalization_295\n",
      "748 activation_340\n",
      "749 conv2d_310\n",
      "750 batch_normalization_296\n",
      "751 activation_341\n",
      "752 conv2d_308\n",
      "753 conv2d_311\n",
      "754 batch_normalization_294\n",
      "755 batch_normalization_297\n",
      "756 activation_339\n",
      "757 activation_342\n",
      "758 block8_9_mixed\n",
      "759 block8_9_conv\n",
      "760 block8_9\n",
      "761 block8_9_ac\n",
      "762 conv2d_313\n",
      "763 batch_normalization_299\n",
      "764 activation_344\n",
      "765 conv2d_314\n",
      "766 batch_normalization_300\n",
      "767 activation_345\n",
      "768 conv2d_312\n",
      "769 conv2d_315\n",
      "770 batch_normalization_298\n",
      "771 batch_normalization_301\n",
      "772 activation_343\n",
      "773 activation_346\n",
      "774 block8_10_mixed\n",
      "775 block8_10_conv\n",
      "776 block8_10\n",
      "777 conv_7b\n",
      "778 conv_7b_bn\n",
      "779 conv_7b_ac\n",
      "780 global_average_pooling2d_8\n",
      "781 dense_15\n",
      "782 dropout_8\n",
      "783 dense_16\n"
     ]
    }
   ],
   "source": [
    "for i,layer in enumerate(inres_model.layers):\n",
    "    print(i,layer.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the ensemble model using the last \"dense layer\" of each base CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "model1=Model(inputs=[xception_model.layers[0].get_input_at(0)],outputs=xception_model.get_layer('dense_6').output,name='xception')\n",
    "model2=Model(inputs=[vgg_model.layers[0].get_input_at(0)],outputs=vgg_model.get_layer('dense_8').output,name='vgg')\n",
    "model3=Model(inputs=[vgg19_model.layers[0].get_input_at(0)],outputs=vgg19_model.get_layer('dense_10').output,name='vgg19')\n",
    "model4=Model(inputs=[incep_model.layers[0].get_input_at(0)],outputs=incep_model.get_layer('dense_14').output,name='incep')\n",
    "model5=Model(inputs=[inres_model.layers[0].get_input_at(0)],outputs=inres_model.get_layer('dense_16').output,name='inres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot the figures\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # acc\n",
    "            plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "            # loss\n",
    "            plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble_history= LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23382 images belonging to 5 classes.\n",
      "Found 5845 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "#generate training and test images\n",
    "TARGET_SIZE=(224,224)\n",
    "INPUT_SIZE=(224,224,3)\n",
    "BATCHSIZE=128\t#could try 128 or 32\n",
    "\n",
    "#Normalization\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './train_224/',\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCHSIZE,\n",
    "        class_mode='categorical')\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        './test_224/',\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCHSIZE,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lr_decay(epoch):\n",
    "    lrs = [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0001,0.00001,0.000001,\n",
    "           0.000001,0.000001,0.000001,0.000001,0.0000001,0.0000001,0.0000001,0.0000001,0.0000001,0.0000001\n",
    "          ]\n",
    "    return lrs[epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auto_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)\n",
    "my_lr = LearningRateScheduler(lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ensemble(num_class,epochs,savepath='./ensemble.h5'):\n",
    "    img=Input(shape=(224,224,3),name='img')\n",
    "    feature1=model1(img)\n",
    "    feature2=model2(img)\n",
    "    feature3=model3(img)\n",
    "    x=concatenate([feature1,feature2,feature3])\n",
    "    x=Dropout(0.5)(x)\n",
    "    x=Dense(64,activation='relu')(x)\n",
    "    x=Dropout(0.25)(x)\n",
    "    output=Dense(num_class,activation='softmax',name='output')(x)\n",
    "    model=Model(inputs=img,outputs=output)\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    #train model\n",
    "    earlyStopping=kcallbacks.EarlyStopping(monitor='val_acc',patience=2, verbose=1, mode='auto')\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(filepath=savepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "    hist=model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        callbacks=[earlyStopping,saveBestModel,ensemble_history,auto_lr],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.5297 - acc: 0.8809Epoch 00001: val_acc improved from -inf to 1.00000, saving model to ./ensemble.h5\n",
      "183/183 [==============================] - 204s 1s/step - loss: 0.5278 - acc: 0.8814 - val_loss: 0.0590 - val_acc: 1.0000\n",
      "Epoch 2/20\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1287 - acc: 0.9770Epoch 00002: val_acc did not improve\n",
      "183/183 [==============================] - 184s 1s/step - loss: 0.1286 - acc: 0.9770 - val_loss: 0.0103 - val_acc: 1.0000\n",
      "Epoch 3/20\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9778Epoch 00003: val_acc did not improve\n",
      "183/183 [==============================] - 184s 1s/step - loss: 0.0973 - acc: 0.9778 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 00003: early stopping\n"
     ]
    }
   ],
   "source": [
    "ensemble_model=ensemble(num_class=5,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble_model=load_model('./ensemble.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ./test_224/0\\100015.png\n"
     ]
    }
   ],
   "source": [
    "#read images from validation folder\n",
    "rootdir = './test_224/'\n",
    "test_laels = []\n",
    "test_images=[]\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        if not (file.endswith(\".jpeg\"))|(file.endswith(\".jpg\"))|(file.endswith(\".png\")):\n",
    "            continue\n",
    "        test_laels.append(subdir.split('/')[-1])\n",
    "        test_images.append(os.path.join(subdir, file))\n",
    "        \n",
    "print(test_laels[0],test_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing time is :54.200418 seconds\n"
     ]
    }
   ],
   "source": [
    "#test the averaging model on the validation set\n",
    "import time\n",
    "predict=[]\n",
    "length=len(test_images)\n",
    "t1 = time.time()\n",
    "for i in range((length//127)+1):\n",
    "    inputimg=test_images[127*i:127*(i+1)]\n",
    "    test_batch=[]\n",
    "    for path in inputimg:\n",
    "        thisimg=np.array(Image.open(path))/255\n",
    "        test_batch.append(thisimg)\n",
    "    #print(i, np.array(test_batch).shape)\n",
    "    ensemble_model_batch=ensemble_model.predict(np.array(test_batch))\n",
    "    predict_batch=list(np.argmax(ensemble_model_batch,axis=1))\n",
    "    predict_batch=[label[con] for con in predict_batch]\n",
    "    predict.append(predict_batch)\n",
    "\n",
    "predict=sum(predict,[])\n",
    "\n",
    "t2 = time.time()\n",
    "print('The testing time is :%f seconds' % (t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenation accuracy:1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "acc=accuracy_score(test_laels,predict)\n",
    "print('Concatenation accuracy:%s'%acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5052    0    0    0    0]\n",
      " [   0  225    0    0    0]\n",
      " [   0    0  200    0    0]\n",
      " [   0    0    0  197    0]\n",
      " [   0    0    0    0  171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5052\n",
      "           1       1.00      1.00      1.00       225\n",
      "           2       1.00      1.00      1.00       200\n",
      "           3       1.00      1.00      1.00       197\n",
      "           4       1.00      1.00      1.00       171\n",
      "\n",
      "    accuracy                           1.00      5845\n",
      "   macro avg       1.00      1.00      1.00      5845\n",
      "weighted avg       1.00      1.00      1.00      5845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_laels, predict))\n",
    "target_names = ['0', '1','2','3','4']\n",
    "print(classification_report(test_laels, predict, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
